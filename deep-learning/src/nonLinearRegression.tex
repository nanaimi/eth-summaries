\section*{Nonlinear Regression}
basis expansion
\textbf{Idea:} Feature space transformation\\
Model: $\mathbf{Y}=f(\mathbf{X})=\sum_{i=1}^d\w_i \phi_i(\x)$\\
Transformation $\phi_i(\mathbf{X}):\mathbb{R}^d \rightarrow \mathbb{R}$


\iffalse
\subsection*{Cubic Spline}
 e.g. for $d=1$ with knots at $\xi_1$ and $\xi_2$\\
 $h_1(X){=}1\ \ \ h_3(X){=}X^2\ \ \ h_5(X){=}(X{-}\xi_1)^3_+$ \ \ \
 $h_2(X){=}X \quad h_4(X){=}X^3\ \ \ h_6(X){=}(X{-}\xi_2)^3_+$

 \subsection*{Smoothing Spline}
 Add penalty for second deriv.:  
 $RSS(f, \lambda) {=}$
 $ \sum_{n=1}^n (y_i-f(x_i))^2 + \lambda\int (f''(x))^2 dx$
\subsection*{Wavelets}
 Functions that measure local properties of the underlying data. Keep the most important ones and get rid of the noise.


\subsection*{Gaussian Process Regression}
joint Gaussian over all outputs
$\mathbf{y}=\mathbb{X}\beta+\epsilon$\\  $\epsilon\sim \mathcal{N}(0,\sigma^2\mathbb{I}_n)$;$\E[\begin{bmatrix}
\mathbf{y}\\
y_{n+1}\\
\end{bmatrix}]$;$\V[\begin{bmatrix}
\mathbf{y}\\
y_{n+1}\\
\end{bmatrix}]$\\
We can rewrite the distribution
$p(\begin{bmatrix}
\mathbf{y}\\
y_{n+1}\\
\end{bmatrix})\\{=}\mathcal{N}(\mathbf{0},\begin{bmatrix}
(\mathbf{K}+\sigma^2\mathbb{I}) & \mathbf{k} \\
\mathbf{k}^T & k(x_{n+1},x_{n+1})+\sigma^2 \\
\end{bmatrix})$\\
Lengthscale: how far can reliably extrapol.
\subsection*{Gaussian Process Prediction}
$p(y_{n+1}|x_{n+1},\mathbf{X},\mathbf{y}) = \mathcal{N}(\mu_{y_{n+1}},\sigma^2_{y_{n+1}})$,\\
$\mu_{y_{n+1}}=\mathbf{k}^T(\mathbf{K}+\sigma^2\mathbb{I})^{-1}\mathbf{y}$,
$\sigma^2_{y_{n+1}}=k(x_{n+1},x_{n+1}){+}\sigma^2-\mathbf{k}^T(\mathbf{K}+\sigma^2\mathbb{I})^{-1}\mathbf{k}$\\
$\mathbf{k}=k(x_{n+1},\mathbf{X})\quad \mathbf{K}_{ij}=k(x_i,x_j)$\\
\subsection*{General Conditional Gaussian Pred}
$P(\begin{bmatrix}
\mathbf{a1}\\
\mathbf{a2}\\
\end{bmatrix}){=}\mathcal{N}(\begin{bmatrix}
\mathbf{a1}\\
\mathbf{a2}\\
\end{bmatrix}|\begin{bmatrix}
\mathbf{u1}\\
\mathbf{u2}\\
\end{bmatrix},\begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}\\
\end{bmatrix})$\\
 $p(\mathbf{a_1}|\mathbf{a_2} = z) = \mathcal{N}(\mathbf{a_2}|\mathbf{u}+\Sigma_{21} \Sigma_{11}^{-1}(\mathbf{z}-\mathbf{u_1}, \Sigma_{22}- \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12})$
\fi
\section*{Bias-Variance tradeoff}
Bias($\hat{f}$)$=\mathbb{E}[\hat{f}]-f$\\
Var($\hat{f}$)$=\mathbb{E}[(\hat{f}-\mathbb{E}[\hat{f}])^2]$\\
$|\mathcal{Z}|\downarrow \quad|\mathcal{F}|\uparrow\quad\Rightarrow\quad\mathrm{Var}\uparrow\quad\mathrm{Bias}\downarrow $\\
$|\mathcal{Z}|\uparrow \quad|\mathcal{F}|\downarrow\quad\Rightarrow\quad\mathrm{Var}\downarrow\quad\mathrm{Bias}\uparrow $

\subsection*{Squared Error Decomposition}
$\mathbb{E}_D\mathbb{E}_{X,Y}[(\hat{f}(X)-Y)^2]=$\\
$\mathbb{E}_{X,Y}[(\mathbb{E}_{Y|X}[Y]-Y)^2]$ (noise$^2$)\\
$+\mathbb{E}_X\mathbb{E}_D[(\hat{f}_D(X)-\mathbb{E}_D[\hat{f}(X)])^2]$ (var.)\\
$+\mathbb{E}_X[(\mathbb{E}_D[\hat{f}_D(X)]-\mathbb{E}_{Y|X}[Y])^2]$ (bias$^2$)\\
With $\mathbb{E}_{Y|X}[Y]$ the expected label and $\mathbb{E}_{D}[\hat{f}(X)$ the expected classifier.