\section*{Chapter 18: Variational Autoencoders}
%\section*{Probabilities}
%\subsection*{Expect, Var, Cov, Bay}
%$\E[X]=\int_{\Omega}xf(x)\di x=\int_{\omega}x\Prob[X{=}x]\di x$ \\
\textbf{Definetti's Thm}: for exchangeable data, we can decompose it by a latent variable model $p(x_1, \hdots, x_N) = \int\prod_{i=1}^{N}p_\theta(x_i|z)p_\theta(z)dz$. Problem: assumption of ind.

Clasically, define complex models via marginalization of a latent variable model: $p_\theta(x) = \int p_\theta(x,z)dz$. This posterior is computed using sampling methods or variational inference (ELBO).

\subsection*{Variational autoencoders}
Generalize factor analysis with depth. Given a "noise" variable $z \sim N(0, \mathbb I)$, propagate through DNN s.t. $\x = F(z) = (F^L \circ \hdots \circ F^1)(z)$. Encoder $\x \xMapsto[]{q_\phi} (\mu, \Sigma)$ and decoder $\z \xMapsto[]{p_\theta} \tilde{x}$ where $\z\sim (\mu, \Sigma)$.

To learn F(z), cannot use MLE because DNN does not provide density. We use a DNN that approximates the posterior $p_\theta(\z | \x) \approx q_\phi(\z | \x)$. The marginal log-likelihood (to be max) is $p_\theta(\x^{(1)}, \hdots, \x^{(N)}) = \sum_{i=1}^N \log p_\theta(\x^{(i)})$ where $\log p_\theta(\x^{(i)}) = KL(q_\phi(\z|\x^{(i)}) \| p_\theta(\z|\x^{(i)})) + \mathcal L(\theta, \phi; \x^{(i)})$. The first term is intractable but 
$KL \geq 0 \to$, max the expression is equiv to max the second term (ELBO):

$\log p_\theta(\x^{(i)}) \geq \mathcal L(\theta, \phi; \x^{(i)}) = \E_{q_\phi(\z|\x)}[\log p_\theta(\x, \z) - \log q_\phi(\z|\x)] = \E_{q_\phi(\z|\x^{(i)})}[\log p_\theta(\x^{(i)} | \z)] - KL(q_\phi(\mathbf z | \x^{(i)}) \| p_\theta(\mathbf z))$

\textbf{General Stoch Grad Variational Bayes}

$\mathcal{\tilde{L}}^A(\theta, \phi; \x^{(i)}) = \frac{1}{L}\sum_{l=1}^L\log p_\theta(\x^{(i)}, \z^{(i, l)}) - \log q_\phi(\z^{(i,l)}|\x^{(i)})$ with $\z^{(i,l)} = g_\phi(\epsilon^{(i,l)}, \x^{(i)})$ and $\epsilon^{(l)} \sim p(\epsilon)$