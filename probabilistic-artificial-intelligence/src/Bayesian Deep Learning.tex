\subsection*{BNN} Prior NN $\theta$; NN parametrizes likelihood

$\mu$ \& log $\sigma^{2}$: \; $p(y|\mathbf{x}, \theta ) = \mathcal{N}(y;f_{1}(\mathbf{x}, \theta), exp(f_{2}(\mathbf{x}, \theta)))$

\vspace*{-0.5mm}
MAP/SGD: $\hat{\theta} = amin_\theta - \log p(\theta) - \sum_{i} \log p(y_i | x_i, \theta)$

$\rightarrow$ Handles heteroscedastic noise well, fails to \\
predict epistemic uncertainty $\rightarrow$ use VI

\vspace*{-0.5mm}
\textbf{VI(BbB):} \; SGD-opt ELBO via $\nabla_\lambda L(\lambda)$. Find VI \\
approx $q_\lambda$. Draw $m$ weights $\theta^{(j)} \sim q_\lambda(\cdot)$. Predict \mhl{$p(y^* | x^*, x_{1:n}, y_{1:n}) \approx \frac{1}{m} \sum_j p(y^* | x^*, \theta^{(j)})$}

\vspace*{-0.5mm}
\textbf{MCMC:} \; Produce seq. of weights {\fontsize{9}{6}\selectfont $\theta^{(1)},..,\theta^{(T)}$} via SGLD, LD, SG-HMC; predict by avg. weights.

%Summarize $\theta^{(i)}$ by subsampling or Gaussian approx.