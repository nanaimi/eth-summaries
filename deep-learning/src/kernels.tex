\section*{Kernels $K(\mathbf{x}, \mathbf{x'}) {=} \phi(\mathbf{x})^T\phi(\mathbf{x'})$}
Similarity based reasoning\\
Gram Matrix $K{=}K(\mathbf{x}_i, \mathbf{x}_j), 1{\leq} i,j{\leq} n$\\
sym. p.s.d.(all EV $\geq$ 0); Engineering:\\
$K_1(\mathbf{x}, \mathbf{x'})K_2(\mathbf{x}, \mathbf{x'})$; 
$\alpha K_1(\mathbf{x}, \mathbf{x'})+\beta K_2(\mathbf{x}, \mathbf{x'})$\\
$\phi:\mathcal{X}{\rightarrow}\mathcal{X},\,\,K(\phi(\mathbf{x}), \phi(\mathbf{x'}))$; $p(h|\mathbf{x})p(h'|\mathbf{x'})$\\ 
$h: \mathrm{poly/exp},\,\, h(K(\mathbf{x}, \mathbf{x'}))$;$ \, f(\mathbf{x})K(\mathbf{x},\mathbf{x'})f(\mathbf{x'})$

lin/poly: $\mathbf{x}^T\mathbf{x'}$; $(\mathbf{x}^T\mathbf{x'}{+}1)^p$;
$\mathrm{dim}=\binom{n+p}{p}$\\
RBF(Gauss): $\exp(-||\mathbf{x}{-}\mathbf{x'}||_2^2/h^2)$\\
lapl: $\exp(-||\mathbf{x}{-}\mathbf{x'}||_1/h)$, $h\uparrow$: regularizat.\\
Sigmoid:$\mathrm{tanh}(\alpha\mathbf{x}^T\mathbf{x'}+c)$\\
not p.s-d eg: $\mathbf{x}{=}[1,-1], \mathbf{x'}{=}[-1,2]$

\subsection*{Reformulating the perceptron}
Ansatz: $w^* \in \operatorname{span}(X) \Rightarrow w = \sum_{j=1}^n \alpha_j y_j x_j$\\
$\alpha^*= \argmin_{\alpha} \frac{1}{n} \sum_{i=1}^n \max(0,$\\$- \sum_{j=1}^n \alpha_j y_i y_j x_i^T x_j)$

\subsection*{Kernelized perceptron and SVM}
Use $\alpha^T k_i$ instead of $w^T x_i$,\\
use $\alpha^T D_y K D_y \alpha$ instead of $||w||_2^2$\\ 
$k_i=[y_1 k(x_i,x_1), ..., y_n k(x_i,x_n)]$;\\
$D_y = \text{diag}(y)$\\
Prediction: $f(\hat{x}) = \operatorname{sign}(\sum_{i=1}^n \alpha_i y_i k(x_i, \hat{x}))$

\subsection*{Kernelized linear regression (KLR)}
Ansatz: $w^*=\sum_{i = 1}^n \alpha_i x$\\
$\alpha^*= \argmin_{\alpha}\frac{1}{n} ||\alpha^T K - y||_2^2 + \lambda \alpha^T K \alpha \\= (K+\lambda I)^{-1} y$\\
Prediction: $f(\hat{x}) = \sum \limits_{i=1}^n \alpha_i k(x_i,\hat{x})$